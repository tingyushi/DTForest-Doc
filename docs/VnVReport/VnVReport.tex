\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{float}
\usepackage{geometry}
\usepackage[dvipsnames]{xcolor}


\geometry{margin = 0.75in}

\input{Comments}
\input{Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
March 6th & 1.0 & First Draft\\
\hline
April 2 & 2.0 & Final Version \\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}
The following are some naming conventions and definitions from SRS:
\begin{itemize}
\item LiDAR: Light Detection and Ranging(Scanning Technology)
\item Plot: A square-shaped area in the forest. There are 14 plots in total. 
\item LAI: Leaf Area Index
\item DBH: Diameter at Breast Height
\item Target Forest: Target forest refers to the natural forest located at Turkey
Point in Ontario.
\item Digital Twin Forest: The virtual representation of the target forest.
\item Forest Data: Forest Data include environmental data and tree parameters.
\item Environmental Data:
\begin{itemize}
    \item Humidity
    \item Temperature
    \item Soil Carbon Content
    \item Soil Nitrogen Content
    \item LAI 
\end{itemize}
\item Tree parameters: 
\begin{itemize}
    \item Density
    \item Height
    \item Age
    \item DBH 
\end{itemize}
\item Tree types. There are 7 different types of trees,
the following are the details:
\begin{itemize}
    \item Red Pine
    \item Oak
    \item Birch 
    \item Beech
    \item White Pine
    \item Red Maple
    \item Red Oak
\end{itemize}
\end{itemize}

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}
%%%%%%%%%%%%%%%%%%%%%%%%%%% SRS VnV %%%%%%%%%%%%%%%%%%%%%%%
\section{SRS Verification}
\begin{itemize}
\item All the functional and non-functional requirements  have
been achieved.
\item You can check the latest SRS \href{https://github.com/wuj187/DigitalTwinCAS/blob/main/docs/SRS/SRS.pdf}{here}.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%% SRS VnV Ends %%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%% Design Doc VnV %%%%%%%%%%%%%%%%%%%%%%%%
\section{Design Documentation Verification}
According to the SRS verification documented above, Design documentation is
revised and modified. The major modifications are listed below:

\begin{itemize}
\item We rearranged the data categories and hierarchy, and removed the overall
forest view. For environmental data, keep data \verb|Humidity|, 
\verb|Soil Carbon Content|, \verb|Soil Nitrogen Content|, \verb|Temperature|,
and \verb|LAI|. We added a pie chart to show the percentages of each tree 
type. For tree parameters, we kept data \verb|Density|, \verb|DBH|,
\verb|Height|, \verb|Age|. 
Overall forest view is combined with the models for each plot instead, and the
related information can still be reached.

\item Change the stored models to the instantly generated models. Models for 
14 plots are both space-consuming and redundant for our product. Store the
constraints and methods to generate models instantly, instead of the models
themselves.


\item Remove the information for each tree. Instead, show the information
about each kind of trees for each plot. This modification is for better
accomplishing the scientific purpose, as the scientific work of our
stakeholders concerns about the overall information within a certain area,
instead of specific individuals. Also, due to limited resources and schedule,
precisely reappearing every detail of the target forest in the model is not 
realistic. 

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%% Design Doc VnV ends %%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%% VnV Plan VnV %%%%%%%%%%%%%%%%%%%%%%%%
\section{VnV Plan Verification}
During the test processes, the Forest Mirror team performed to verify the
implementations of functional requirements, team members found that the 
Automated Testing and Verification Tool section is not correct because 
automated testing is not applicable to the application. The back-end code of
the application is bound to the game components in Unity, which can not be 
tested by solely using automated tools and running it so the team used manual
tests on Unity to verify the implementations of requirements. After the 
mistake was discovered, the team decided to delete this section from the VnV
plan.
The Forest Mirror team gathered some feedback and problems occurring in the
VnV plan from the other team members and TAs and modified some contents in the
presenting version of the VnV plan according to that feedback.\\

\noindent
\textcolor{red}{VnV plan has been modified according to the above
result.}
%%%%%%%%%%%%%%%%%%%%%% VnV Plan VnV ends %%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%% Implementation Verification %%%%%%%%%%%%%%%%
\section{Implementation Verification}
Our team deployed both dynamic and static testing to verify the 
implementation of our project. For dynamic testing, since we implement 
the software with Unity, we used integration testing and system testing
most of the time to monitor each component to operate normally. Also,
the tests cases are evaluated to make sure all lines of code are 
covered. All the dynamic testing was done without any error. \\
\noindent
During the development of the project, we constantly communicated with
our stakeholder ---- Dr.Gonsamo to ensure that we are doing the right
product. Also, by demonstrating the proof of concept to stakeholders,
we can ensure that our clients will be satisfied with our product.\\
\noindent
For static testing, our team used code inspection and document
inspection to make sure that the design is following our
expectations (high cohesion and low coupling, easy to modify) and there
is no big issue in the code. Our purpose for static testing is to make 
sure the code have good quality and complete. After our testing, we evaluated our code with good quality and completeness.
%%%%%%%%%%%%%%%%%%%%%%%%%%%% Implementation Verification Ends %%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Automated Testing %%%%%%%%%%%%%%%%%%%%
\section{Automated Testing and Verification Tools}
All the tests were conducted manually. So this part is not applicable.
%%%%%%%%%%%%%% Automated Testing Ends %%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Software Validation %%%%%%%%%%%%%%%%%%
\section{Software Validation}
After the Rev0 demo, we had a meeting with Dr. Gonsamo, and he gave us the following 
advice:
\begin{itemize}
\item We need to adjust the initial camera position when users just enter the 
forest model. The camera position should allow users to have a full view of the 
plot.
\item We should add another GUI for displaying leaf information.
\item We should implement seasonal change.
\item Tree distribution should follow the real forest tree distribution.
\end{itemize}
%%%%%%%%%%%%%% Software Validation Ends %%%%%%%%%%%%%

\newpage


%%%%%%%%%%%%%%%%%%%%%%%% Functional Requirements Evaluation %%%%%%%%%%%%%%%%%
\section{Functional Requirements Evaluation}
\subsection{Presentation}
\noindent The following testing results of test cases 
related to the presentation of our products.

\newcommand{\pass}{{\color{Green}PASS}}
\newcommand{\fail}{{\color{Red}FAIL}}

%%%%%%%%%%% Test-FR1 %%%%%%%%%%%%%%%%%%%%
\subsubsection{Test-FR1}
\begin{itemize}
\item Inputs: Testers clicked \verb|Instruction| button
for 10 times and reset the system to the initial 
state(Main page) after each click.
\item Excepted Value: Instruction page should appear 
on the screen 10 times.
\item Actual Value: Instruction page appeared on the 
screen 10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%% Test-FR2 %%%%%%%%%%%%%%%%%%%%
\subsubsection{Test-FR2}
\begin{itemize}
\item Inputs: Testers clicked \verb|Contact Us| button
for 10 times and reset the system to the initial 
state(Main page) after each click.
\item Excepted Value: Contact Us page should appear 
on the screen 10 times.
\item Actual Value: Contact Us page appeared on the 
screen 10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%% Test-FR3 %%%%%%%%%%%%%%%%%%%%
\subsubsection{Test-FR3}
\begin{itemize}
\item Inputs: Testers clicked \verb|Start| button
for 10 times and reset the system to the initial 
state(Main page) after each click.
\item Excepted Value: Full view of forest plot 1 should appear on
the screen 10 times.
\item Actual Value: Full view of forest plot 1 appeared on the screen 
10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%% Test-FR7.1 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR7.1}
\begin{itemize}
\item Inputs: Testers clicked \verb|Environmental Data| button
10 times and reset the system to the initial state after each click.
The initial state is defined as the forest model presented but
environmental data are not displayed.
\item Excepted Value: Environmental data should appear on the left side
of the screen 10 times
\item Actual Value: Environmental data appeared on the left side of the
screen 10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR7.2 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR7.2}
\begin{itemize}
\item Inputs: Testers click \verb|Tree Parameters| button
10 times and reset the system to the initial state after each click.
The initial state is defined as the forest model presented but tree 
parameters are not displayed.
\item Excepted Value: Tree parameters should appear on the right side
of the  screen 10 times
\item Actual Value: Tree parameters appeared on the right side of the
screen  10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR10.1 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR10.1}
\begin{itemize}
\item Inputs: Testers recorded all the environmental data from
the GUI. This includes environmental data for 14 plots and the overall 
forest. 
\item Expected Value: All the environmental data recorded from the
GUI should match the JSON files stored 
\href{https://github.com/tingyushi/DTForest-DS}{here}.
\item Actual Value: All the environmental data recorded from the
GUI matches the JSON files.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR10.2 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR10.2}
\begin{itemize}
\item Inputs: Testers record all the tree parameters from
the GUI. This includes tree parameters of 7 types of trees from  14
plots and the overall forest. 
\item Expected Value: All the tree parameters recorded from the
GUI should match the JSON files stored 
\href{https://github.com/tingyushi/DTForest-DS}{here}.
\item Actual Value: All the tree parameters recorded from the
GUI matches the JSON files.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR13 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR13}
\begin{itemize}
\item Inputs: Testers updated tree parameters(including 
tree density, tree height, tree DBH) for all 7 types of trees 
in 14 plots.
\item Expected Value: Tree models should change according to the 
data update.
\item Actual Value: Tree models changed according to the data update.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR14 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR14}
\begin{itemize}
\item Inputs: Testers clicked on \verb|Seasonal Change| button 
10 times in succession.
\item Expected Value: Summer season and winter season should appear
5 times, respectively.
\item Actual Value:  Summer season and winter season appeared
5 times, respectively.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR15 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR15}
\begin{itemize}
\item Inputs: Testers clicked on \verb|Switch| button 10 times and 
reset to the initial state after each click event. 
\item Expected Value: The pie chart should appear on the left
side of the screen 10 times.
\item Actual Value:  The pie chart appeared on the left
side of the screen 10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR16 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR16}
\begin{itemize}
\item Inputs: Testers got the top view of each plot. 
\item Expected Value: The virtual tree model distributions 
should be consistent with satellite pictures.
\item Actual Value:  The virtual tree model distributions are
consistent with satellite pictures.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Users' interaction with the product}
%%%%%%%%%%%%%%% Test-FR4.1 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR4.1}
\begin{itemize}
\item Inputs: Testers clicked \verb|Environmental Data| button 10 times 
and reset the system to the initial state after each click. The initial
state is defined as ``environmental data are displayed''
\item Expected Value: Environmental data display should disappear 10 times.
\item Actual Value: Environmental data display disappeared 10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR4.2 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR4.2}
\begin{itemize}
\item Inputs: Testers clicked \verb|Tree Parameters| button 10 times
and reset  the system to the initial state after each click. The
initial state is defined as ``tree parameters are displayed''
\item Expected Value: Tree parameters display should disappear 10 times.
\item Actual Value: Tree parameters display disappeared 10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR5 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR5}
\begin{itemize}
\item Inputs: Testers clicked \verb|Environmental Data| button
10 times and reset the system to the initial state after each click.
The initial state is defined as the forest model presented but
environmental data are not displayed.
\item Excepted Value: Environmental data should appear on screen 10
times.
\item Actual Value: Environmental data appeared on the screen 10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Test-FR6 %%%%%%%%%%%%%%%%%
\subsubsection{Test-FR6}
\begin{itemize}
\item Inputs: Testers clicked \verb|Tree Parameters| button
10 times and reset the system to the initial state after each click.
The initial state is defined as the forest model presented but tree 
parameters are not displayed.
\item Excepted Value: Tree parameters should appear on screen 10 
times.
\item Actual Value: Tree parameters appeared on the screen  10 times.
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Test-FR8 %%%%%%%%%%%%%%%%%%%%
\subsubsection{Test-FR8}
\begin{itemize}
\item Inputs: Testers pressed \verb|W| key,  \verb|A| key, 
, \verb|S| key, and \verb|D| key.
\item Expected value: 
\begin{itemize}
\item Users’ first view should move forward after pressing \verb|W|
key.
\item Users’ first view should move left after pressing \verb|A|
key.
\item Users’ first view should move backward after pressing \verb|S|
key.
\item Users’ first view should move right after pressing \verb|D|
key.
\end{itemize}
\item Actual value: 
\begin{itemize}
\item Users’ first view moved forward after pressing \verb|W|
key.
\item Users’ first view moved left after pressing \verb|A|
key.
\item Users’ first view moved backward after pressing \verb|S|
key.
\item Users’ first view moved right after pressing \verb|D|
key.
\end{itemize}
\item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Test-FR9 %%%%%%%%%%%%%%%%%%%%
\subsubsection{Test-FR9}
\begin{itemize}
\item Inputs: Testers moved the mouse in 4 different directions, which
are left, right, forward, and backward.
\item Expected value: The user's point of view should rotate according to the input. 
\begin{itemize}
\item Move the mouse forward: The user's first point of view should rotate up.
\item Move the mouse backward: The user's first point of view should rotate 
down.
\item Move the mouse left: The user's first point of view should rotate left.
\item Move the mouse right: The user's first point of view should rotate right.
\end{itemize}

    \item Actual value: The user's point of view rotated according to the input. 
    \begin{itemize}
    \item Move the mouse forward: The user's first point of view rotated up.
    \item Move the mouse backward: The user's first point of view rotated down.
    \item Move the mouse left: The user's first point of view rotated left.
    \item Move the mouse right: The user's first point of view rotated right.
\end{itemize}
    \item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Test-FR11 %%%%%%%%%%%%%%%%%%%%
\subsubsection{Test-FR11}
\begin{itemize}
    \item Inputs: Testers clicked \verb|Quit| button located in the main page.
    \item Expected value: The product should terminate.
    \item Actual value: The product terminated.
    \item Result: \pass
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Test-FR12%%%%%%%%%%%%%%%%%%%%
\subsubsection{Test-FR12}
\begin{itemize}
\item Inputs: Testers entered new data in the input box and clicked 
\verb|Update| button.
\item Expected value: A text should pop up indicating if the update was
successful. And environmental data display and tree parameters display
should reflect the newly entered data. 
\item Actual value: A text popped up indicating if the update was successful. And environmental data display and tree parameters display  
reflected the newly entered data.
\item Result: \pass
\end{itemize}
\textcolor{blue}{NOTE: The above test was conducted for 14 plots and 7 tree types. 
All tests passed.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%% Functional Requirements Evaluation Ends %%%%%%%%%%%%%%%%%

\newpage

%%%%%%%%%%%%%%%%%%%%%%%% Non-Functional Requirements Evaluation Ends %%%%%%%%%
\section{Nonfunctional Requirements Evaluation}

\subsection{Look and Feel Requirements Evaluation}
\subsubsection{Test-NFR-LF1.1}
\begin{itemize}
    \item Testing Process: We conducted interviews with random people to gather feedback by providing the questionnaire in the Appendix.
    \item Expected Result: Over 80 percent of the users choose A or B in the first question in the questionnaire.
    \item Actual Result: Out of 20 interviewed users, 10 chose A and 7 chose B in this question.(17/20)
    \item Result Analysis: \pass  
\end{itemize}
            \subsubsection{Test-NFR-LF2.1}
\begin{itemize}
    \item Testing Process: Testers compared the forest data stored in the software with physical measurements and calculate the relative errors of the data
    \item Expected Result: All the forest data have errors less than 0.1 compared to the actual data our clients provide us
    \item Actual Result: The data has relative errors less than 0.1.
    \item Result Analysis: \pass
\end{itemize}
            \subsubsection{Test-NFR-LF2.2}
\begin{itemize}
    \item Testing Process: We conducted interviews with random people to gather feedback by providing the questionnaire in the Appendix.
    \item Expected Result: Over 80 percent of the users choose A or B in the second question in the questionnaire.
    \item Actual Result: Out of 20 interviewed users, 9 chose A and 9 chose B in this question.(18/20)
    \item Result Analysis: \pass
\end{itemize}
\subsection{Usability and Humanity Requirements Evaluation}
            \subsubsection{Test-NFR-UH1.1}
\begin{itemize}
    \item Testing Process: We conducted interviews with random people to gather feedback by providing the questionnaire in the Appendix.
    \item Expected Result: Over 80 percent of the users choose A or B in the third question in the questionnaire.
    \item Actual Result: Out of 20 interviewed users, 5 chose A and 12 chose B in this question.(17/20)
    \item Result Analysis: \pass
\end{itemize}

\subsubsection{Test-NFR-UH2.1}
\begin{itemize}
    \item Testing Process: Every member of our team inspected all the UI components that contain text in the software to see if there are any non-English texts.
    \item Expected Result: 100 percent of the texts should be used in English except for numbers.
    \item Actual Result: 100 percent of the texts are written in English except for numbers.
    \item Result Analysis: \pass
\end{itemize}

            \subsubsection{Test-NFR-UH3.1}
\begin{itemize}
    \item Testing Process: Testers launched the system, went to the main page and clicked the instructions option.
    \item Expected Result: Instructions are displayed on the screen.
    \item Actual Result: Instructions are displayed on the screen.
    \item Result Analysis: \pass
\end{itemize}

            \subsubsection{Test-NFR-UH4.1}
\begin{itemize}
    \item Testing Process: We conducted interviews with random people to gather feedback by providing the questionnaire in the Appendix.
    \item Expected Result: Over 80 percent of the users choose A or B in the fourth question in the questionnaire.
    \item Actual Result: Out of 20 interviewed users, 12 chose A and 7 chose B for this question.(19/20)
    \item Result Analysis: \pass
\end{itemize}

\subsubsection{Test-NFR-UH4.2}
\begin{itemize}
    \item Testing Process: We conducted interviews with random people to gather feedback by providing the questionnaire in the Appendix.
    \item Expected Result: Over 80 percent of the users choose A or B in the fifth question in the questionnaire
    \item Actual Result: Out of 20 interviewed users, 11 chose A and 9 chose B for this question.(20/20)
    \item Result Analysis: \pass
\end{itemize}

\subsubsection{Test-NFR-UH5.1}
\begin{itemize}
    \item Testing Process: We asked five users to navigate through our system using a mouse and keyboard.
    \item Expected Result: All of them will go through the system without any trouble.
    \item Actual Result: All of them can go through the system without any trouble.
    \item Result Analysis: \pass
\end{itemize}

\subsubsection{Test-NFR-UH5.2}
\begin{itemize}
    \item Testing Process: We conducted interviews with random people to gather feedback by providing the questionnaire in the Appendix.
    \item Expected Result: Over 80 percent of the users choose A or B in the sixth question in the questionnaire.
    \item Actual Result: Out of 20 interviewed users, 15 of them chose A and 1 of them chose B(16/20)
    \item Result Analysis: \pass
\end{itemize}
\subsection{Performance Requirements Evaluation}
\subsubsection{Test-NFR-PR1.1}
\begin{itemize}
    \item Testing Process: Testers launched the software, set a timer, and recorded the time it took to respond to the events from the mouse and the keyboard.
    \item Expected Result: The system should respond to any requests within 2 seconds. 
    \item Actual Result: The average response time is less than 2 seconds.
    \item Result Analysis: \pass 
\end{itemize}
\subsubsection{Test-NFR-PR1.2}
\begin{itemize}
    \item Testing Process: Testers launched the software, moved the mouse, clicked it, and pressed the keyboard. At the same time, they recorded the FPS displayed on the screen and the time.
    \item Expected Result: 80 percent of the time, the software should have at least 30 FPS.
    \item Actual Result: The software has over 30 FPS all the time when it is running.
    \item Result Analysis: \pass 
\end{itemize}
\subsubsection{Test-NFR-PR1.3}
\begin{itemize}
    \item Testing Process: Testers clicked on the start option on the main page and recorded the time the software took to show all the models.
    \item Expected Result: The system should load all the models within 10 seconds.
    \item Actual Result: The system loads all the models in less than 10 seconds.
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-PR3.1}
\begin{itemize}
    \item Testing Process: Testers recorded all the data and parameters displayed in the application and compared the data with the actual data collected by Dr.Gonsamo and his lab members, then testers computed the relative error for each data.
    \item Expected Result: All data should have relative errors less than 0.05.
    \item Actual Result: All the forest data have relative errors less than 0.05.
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-PR4.1}
\begin{itemize}
    \item Testing Process: Testers clicked on the application icon and waited for the application to open.
    \item Expected Result: The application should open normally every time when testers click on the application icon.
    \item Actual Result: The application open normally every time when testers click on the application icon.
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-PR4.2}
\begin{itemize}
    \item Testing Process: Testers launched the software and ran the application in the background for 24 hours.
    \item Expected Result: The application should not crash in 24 hours.  
    \item Actual Result: The application runs normally and does not crash in 24 hours.
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-PR5.1}
\begin{itemize}
    \item Testing Process: Testers ran the application without an internet connection, then clicked all the buttons in the system, pressed the keyboard keys, moved the mouse on the forest page, and checked if the system responded correctly to the mouse and the keyboard inputs. 
    \item Expected Result: the system should respond correctly to the mouse and the keyboard inputs. 
    \item Actual Result: the system responds correctly to the mouse and the keyboard inputs. 
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-PR6.1}
\begin{itemize}
    \item Testing Process: Testers opened the property window of the application and checked its size.
    \item Expected Result: The size of the software should be less than 10 GB. 
    \item Actual Result: The size of the software is around 250MB.
    \item Result Analysis: \pass
\end{itemize}


\subsection{Operational and Environmental Requirements Evaluation}
\subsubsection{Test-NFR-OE1.1}
\begin{itemize}
    \item Testing Process: Testers ran the application on both desktops and laptops to check if it worked normally.
    \item Expected Result: The application should ran normally on desktops and laptops.
    \item Actual Result: The application runs normally on desktops and laptops.
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-OE1.2}
\begin{itemize}
    \item Testing Process: Testers ran the software and performed all kinds of actions in the software using a mouse and keyboard.
    \item Expected Result: All actions should be accomplished without any problems.
    \item Actual Result: All actions are accomplished without any problems.
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-OE2.1}
\begin{itemize}
    \item Testing Process: Testers ran the software on Windows 10 or later version or MacOS 12 or later version and performed operations on the software.
    \item Expected Result: All the operations should be accomplished normally.
    \item Actual Result: All the operations are accomplished normally
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-OE3.1}
\begin{itemize}
    \item Testing Process: Testers downloaded and installed the application from GitHub and checked if it could run normally.
    \item Expected Result: The application should be downloaded successfully from GitHub to the computer, and it can run without any errors and bugs.
    \item Actual Result: The application is downloaded successfully from GitHub to the computer and runs without any errors or bugs.
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-OE4.1}
\begin{itemize}
    \item Testing Process: Testers looked at the git commit history and checked if there were weekly updates.
    \item Expected Result: There should be weekly updates in the git commit history.
    \item Actual Result: There are weekly updates in the git commit history.
    \item Result Analysis: \pass
\end{itemize}
\subsubsection{Test-NFR-OE4.2}
\begin{itemize}
    \item Testing Process: Testers performed tests on the previous functions after each update and recorded the test results.
    \item Expected Result: All the previous functions should be unaffected, and all the test cases should be passed.
    \item Actual Result: All the previous functions are unaffected, and all the test cases are passed.
    \item Result Analysis: \pass
\end{itemize}

\subsection{Maintenance and Support Requirements Evaluation}
\subsubsection{Test-NFR-MS1.1}
\begin{itemize}
    \item Testing Process: Testers checked the documentation of the product to see if the new features, functions, and any modifications were added to the documentation. 
    \item Expected Result: All features, functions and modifications should be on the documentation.
    \item Actual Result: New features, functions and modifications after February are not on the documentation.
    \item Result Analysis: \fail

\end{itemize}

\subsubsection{Test-NFR-MS1.2}
\begin{itemize}
    \item Testing Process: Testers read the documentation of the product to see if the documentation specifies the functions clearly.
    \item Expected Result: All functions on the documentation should be clearly documented.
    \item Actual Result: All functions on the documentation are clearly documented.
    \item Result Analysis: \pass
\end{itemize}


\subsubsection{Test-NFR-MS1.3}
\begin{itemize}
    \item Testing Process: Testers recorded the time from the bug occurred to the time when the bug
was fixed and Measured the time to see if it was within three days.
    \item Expected Result: All bugs should be fixed within three days.
    \item Actual Result: All bugs are fixed within three days.
    \item Result Analysis: \pass
\end{itemize}


\subsubsection{Test-NFR-MS2.1}
\begin{itemize}
    \item Testing Process: Testers first looked for the contact method on the contact page,
then used the contact method to contact the developers to see if they could contact the developers
successfully or not.
    \item Expected Result: Testers should be able to reach the developers successfully.
    \item Actual Result: Testers are able to reach the developers successfully.
    \item Result Analysis: \pass
\end{itemize}


\subsubsection{Test-NFR-MS3.1}
\begin{itemize}
    \item Testing Process:  Testers tried to launch the application on more than one operating
system to see if it could be run successfully on a different system or not.
    \item Expected Result: The application should be able to launch on more than one operating system. 
    \item Actual Result: The application is able to launch on more than one operating system. 
    \item Result Analysis: \pass
\end{itemize}


\subsubsection{Test-NFR-MS3.2}
\begin{itemize}
    \item Testing Process: Testers tried to launch the application on the devices located indoors and outdoors respectively to see if the application could be used both indoors and
outdoors.
    \item Expected Result: The application should be able to launch on devices located both indoors and outdoors.
    \item Actual Result: The application is able to launch on devices located both indoors and outdoors.
    \item Result Analysis: \pass
\end{itemize}


\subsection{Security Requirements Evaluation}
\subsubsection{Test-NFR-SR1.1}
\begin{itemize}
    \item Testing Process:  Testers tried to find and download the product from GitHub and other websites.
    \item Expected Result: Testers can not download the product in other websites other than GitHub.
    \item Actual Result: Testers can not download the product in other websites other than GitHub.
    \item Result Analysis: \pass
\end{itemize}

\subsubsection{Test-NFR-SR1.2}
\begin{itemize}
    \item Testing Process:  Testers tried to update the data of trees and forests via the interface and anywhere else to see if they could update the data successfully or not.
    \item Expected Result: Testers should not be able to update the data except through the interface. 
    \item Actual Result: Testers can delete or modify the .json files to modify the data.
    \item Result Analysis: \fail
\end{itemize}




\subsubsection{Test-NFR-SR2.1}
\begin{itemize}
    \item Testing Process:  Testers injected 100 errors on purpose in our application and checked
if the scan results of the computer security application would detect the errors in the computer
system or not.
    \item Expected Result: The number of errors that the product propagates should be less than 2.
    \item Actual Result: 0 error is propagated. 
    \item Result Analysis: \pass
\end{itemize}

\subsubsection{Test-NFR-SR2.2}
\begin{itemize}

    \item Testing Process:  Testers used the software and performed some complicated tasks such as zooming in and zooming out for a long time to see if the application will crash or not.
    \item Expected Result: The application should not crash.
    \item Actual Result: The application does not crash.
    \item Result Analysis: \pass
\end{itemize}

\subsubsection{Test-NFR-SR2.3}
\begin{itemize}

    \item Testing Process:   Testers inputted some invalid data in the \verb|Update Data| page.
    \item Expected Result: Fail to update the data. And a warning message should pop up indicating an invalid update
operation.
    \item Actual Result: Fail to update the data. And a warning message pops up indicating an invalid update
operation.
    \item Result Analysis: \pass
\end{itemize}


\subsubsection{Test-NFR-SR2.4}
\begin{itemize}
    \item Testing Process: Testers manually updated some data into the product, took records, and compared it with the displayed data.
    \item Expected Result: The data should be consistent with the updated data.
    \item Actual Result: The data are consistent with the updated data. 
    \item Result Analysis: \pass
\end{itemize}



\subsubsection{Test-NFR-SR2.5}
\begin{itemize}
    \item Testing Process: Testers manually compared the GUI and data files.  
    \item Expected Result: Each data should have a unique position to display in the GUI, and each GUI should correspond to unique data. 
    \item Actual Result: Each data has one unique position to display in the GUI, and each GUI corresponds to unique data. 
    \item Result Analysis: \pass
\end{itemize}




\subsubsection{Test-NFR-SR3.1}
\begin{itemize}
    \item Testing Process: Testers used a questionnaire to check if the users have been asked to provide personal information when using the product.
    \item Expected Result: All the users choose B in the ninth question in the questionnaire.
    \item Actual Result: No user was asked to provide personal information.
    \item Result Analysis: \pass
\end{itemize}


\subsubsection{Test-NFR-SR3.2}
\begin{itemize}
    \item Testing Process: Testers used a questionnaire to check if the product has sent a notification without permission.
    \item Expected Result: All the users choose B in the seventh question in the questionnaire.
    \item Actual Result: All the users chose B in the seventh question in the questionnaire.
    \item Result Analysis: \pass
\end{itemize}


\subsection{Cultural and Political Requirements Evaluation}
\subsubsection{Test-NFR-CP1.1}
\begin{itemize}
    \item Testing Process: Testers used a questionnaire to check if any users have been offended by the product.
    \item Expected Result: All users chose B for the eighth question in the questionnaire.
    \item Actual Result: All users chose B for the eighth question in the questionnaire.
    \item Result Analysis: \pass
\end{itemize}

\subsection{Legal Requirements Evaluation}
\subsubsection{Test-NFR-LR2.1}
\begin{itemize}
    \item Testing Process: Testers asked the users if any part appeared lawfully unreasonable when spreading the questionnaires.
    \item Expected Result: Users should not notice any part lawful unreasonable.
    \item Actual Result: No user notices any part lawful unreasonable.
    \item Result Analysis: \pass
\end{itemize}

%%%%%%%%%%%%%%%%% NFR Test Ends %%%%%%%%%%%%%%%%%%%%%

\newpage
	
\section{Comparison to Existing Implementation}	
This section will not be appropriate for every project.

\section{Unit Testing}
Unit testing does not accommodate Unity's workflow. Each script is automatically compiled by Unity. If the team wants to test a particular module, they still have to run the whole system and check the Unity console. As a result, all unit testing is included in the system testing. Please check sections 3 and 4 instead.

\section{Changes Due to Testing}
\begin{itemize}
    \item Feedback from the Rev 0 demo: It does not make sense that there is so much grass on the forest floor. The tree heights are unbalanced because no tree shall grow in the shade of other trees.\\
    The team change the ground texture to show more details of the soil and fallen leaves instead of grass for realism. The team measures the accurate height of the tree model, later the models are resized to fit the data collected by drones.
    \item Feedback from the Rev 0 demo: There are spelling and grammar errors in the user interface and codes.\\
    The team goes through the user interface and comments to fix the typo in the project.
    \item Feedback from the Rev o demo: The user interface is not straightforward. It is hard to learn how to update forest data.\\
    The team rearranges the user interface to make it easier to learn.
    \item Feedback from Dr. Gonsamo's lab: The species and tree distribution shall be consistent with the real forest.\\
    The team checked the species counted by Dr. Gonsamo's lab and updated the models. The team later writes a script to dynamically control the distribution of tree models based on satellite photos.
\end{itemize}



\section{Automated Testing}
Automated testing does not accommodate Unity's workflow, everything is tested manually in Unity, such as UI, scene manager, and terrain.etc.

\newpage

\section{Trace to Requirements}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    Function Requirement     &  Test Case ID\\
    \hline
    FR1     & Test-FR1\\
    \hline
    FR2 & Test-FR2\\
    \hline
    FR3 & Test-FR3\\
    \hline
     FR4 & Test-FR4.1, Test-FR4.2\\
    \hline
     FR5 & Test-FR5\\
    \hline
     FR6 & Test-FR6\\
    \hline
     FR7 & Test-FR7.1, Test-FR7.2\\
    \hline
     FR8 & Test-FR8\\
    \hline
     FR9 & Test-FR9\\
    \hline
     FR10 & Test-FR10.1, Test-FR10.2\\
    \hline
     FR11 & Test-FR11\\
    \hline 
     FR12 & Test-FR12\\
    \hline
    FR13 & Test-FR13.1\\
    \hline
    FR14 & Test-FR14\\
    \hline
    FR15 & Test-FR15\\
    \hline
    FR16 & Test-FR16\\
    \hline
    \end{tabular}
    \caption{Traceability between Functional Requirements and Tests}
\end{table}


\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    Non-functional Requirement & Test Case ID\\
    \hline
    NFR-LF1.1 & Test-NFR-LF1.1\\
    \hline
    NFR-LF2.1 & Test-NFR-LF2.1\\
    \hline
    NFR-LF2.2 & Test-NFR-LF2.2\\
    \hline
    NFR-UH1.1 & Test-NFR-UH1.1\\
    \hline
    NFR-UH2.1 & Test-NFR-UH2.1\\
    \hline
    NFR-UH3.1 & Test-NFR-UH3.1\\
    \hline
    NFR-UH4.1 & Test-NFR-UH4.1\\
    \hline
    NFR-UH4.2 & Test-NFR-UH4.2\\
    \hline
    NFR-UH5.1 & Test-NFR-UH5.1\\
    \hline
    NFR-UH5.2 & Test-NFR-UH5.2\\
    \hline
    NFR-PR1.1 & Test-NFR-PR1.1\\
    \hline
    NFR-PR1.2 & Test-NFR-PR1.2\\
    \hline
    NFR-PR1.3 & Test-NFR-PR1.3\\
    \hline
    NFR-PR3.1 & Test-NFR-PR3.1\\
    \hline
    NFR-PR4.1 & Test-NFR-PR4.1\\
    \hline
    NFR-PR4.2 & Test-NFR-PR4.2\\
    \hline
    NFR-PR5.1 & Test-NFR-PR5.1\\
    \hline
    NFR-PR6.1 & Test-NFR-PR6.1\\
    \hline
    \end{tabular}
    \caption{Traceability between Non-Functional Requirements and Tests Part 1}
\end{table}


\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    Non-functional Requirement & Test Case ID\\
    \hline
    NFR-OE1.1 & Test-NFR-OE1.1\\
    \hline
    NFR-OE1.2 & Test-NFR-OE1.2\\
    \hline
    NFR-OE2.1 & Test-NFR-OE2.1\\
    \hline
    NFR-OE3.1 & Test-NFR-OE3.1\\
    \hline
    NFR-OE4.1 & Test-NFR-OE4.1\\
    \hline
    NFR-OE4.2 & Test-NFR-OE4.2\\
    \hline
    NFR-MS1.1 & Test-NFR-MS1.1\\
    \hline
    NFR-MS1.2 & Test-NFR-MS1.2\\
    \hline
    NFR-MS1.3 & Test-NFR-MS1.3\\
    \hline
    NFR-MS2.1 & Test-NFR-MS2.1\\
    \hline
    NFR-MS3.1 & Test-NFR-MS3.1\\
    \hline
    NFR-MS3.2 & Test-NFR-MS3.2\\
    \hline
    NFR-SR1.1 & Test-NFR-SR1.1\\
    \hline
    NFR-SR1.2 & Test-NFR-SR1.2\\
    \hline
    NFR-SR2.1 & Test-NFR-SR2.1\\
    \hline
    NFR-SR2.2 & Test-NFR-SR2.2\\
    \hline
    NFR-SR2.3 & Test-NFR-SR2.3\\
    \hline
    NFR-SR2.4 & Test-NFR-SR2.4\\
    \hline
    NFR-SR2.5 & Test-NFR-SR2.5\\
    \hline
    NFR-SR3.1 & Test-NFR-SR3.1\\
    \hline
    NFR-SR3.2 & Test-NFR-SR3.2\\
    \hline
    NFR-CP1.1 & Test-NFR-CP1.1\\
    \hline
    NFR-LR2.1 & Test-NFR-LR2.1\\
    \hline
    \end{tabular}
    \caption{Traceability between Non-Functional Requirements and Tests Part 2}
\end{table}

\newpage

\section{Code Coverage Metrics}
Code coverage does not accommodate our project since we did not use automated testing.

\newpage

\section{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.  Please answer the following question:\\
\noindent
In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.)

\begin{enumerate}
  \item Bowen Zhang: In general, there can be differences between the VnV plan and the actual activities conducted for VnV due to the testing process of the Unity project. In the team's VnV plan, it is assumed that there will be unit testing and automated testing after completing MG and MIS. However, the team found that each script has been automatically tested by Unity during implementation. When a script is finished, Unity compiles it directly to see if there are any bugs. In addition, the tester needs to run the Unity project to use the script. As a result, unit testing has been combined with system testing. The team shall not use automated testing, everything could be tested manually in Unity, otherwise, it is just a duplication of work. In this way, the team decides to modify the VnV plan. The unit testing section and automated testing section are deleted. The team adds more details in the tests for functional and non-functional requirements to make them more comprehensive. In future projects, this problem can be anticipated by designing a flexible VnV plan that can accommodate Unity workflow. The team should regularly review and update the plan to reflect any changes that happened in Unity. In this way, the team can solve the difference between the VnV plan and the actual activities conducted for VnV and ensure project success.

  \item Yichen Jiang: The major difference between the VnV plan and the activities that were actually conducted for VnV is brought by the change on the design. The VnV report was completed months ago, and within the time till now, our supervisor clarified more ideas that he would love to have in our product. There are many more features now in our product, so the verification and validation activities were adapted for the changes. Besides, the workflow is modified as well. We completed the VnV plan before we worked on MIS and MG, of course also before the real implementation. Our plan used to be to do the testing after all the implementation work's done, but when we implement it, we found that the testing is actually everywhere and anytime. We have to compile the scripts in Unity and check if everything's right all the time, and that is actually how our testing works. This method also influences the unit testing part of our testing plan. Any test has to be done with launching the whole system. Therefore you may notice that we put almost all efforts into system testing in the verification and validation report. 
    \item Junhong Chen: There are some differences between the VnV plan and the actual activities conducted for Vnv. In the VnV plan, we assumed that we can perform unit tests on a single function or a single module. However, in the actual test process, we can't perform unit tests for most of the functions because the implementations of the functions require launching the entire system. For example, it is hard to perform unit tests on the onClick() functions bound to some buttons which capture the click event from the mouse device and opens a new page just by simply running the code itself without launching the entire software. In addition, after several meetings with Dr.Gonsamo and his lab members, a few new features were added to the product, which causes changes in some requirements, leading to the actual activities conducted for VnV differing from the VnV plan.
    \item  Jiacheng Wu: There are a few differences in the two V\&V documents.
     The first one is that the V\&V plan is more about designing the test case and V\&V is about conducting the designed test cases. The second difference is the time. At the time we wrote the V\&V plan, the software was native and we didn't have a clear view of our product but for the report, we have our product completed. Also, during our development, our requirements were changing constantly because we kept contacting with our clients. Due to the change in the requirements, we have to change the SRS as well as the test plan. Our anticipated changes are probably UI and seasonal changes to the model. During this process, I realized that we should constantly change our SRS and V\&V documents every time we contact our clients because our requirements are changing constantly during communication.

    \item Tingyu Shi: VnV Plan is different from the activities that  we conducted
    for the VnV report.  In the VnV plan,
    we documented that we will use automated testing. However, our product 
    involves a lot of GUI and interactions between  users and the software; 
    therefore, manual testing is more feasible and efficient. So, we need to 
    modify the automated testing part for the final document. For a future project,
    if it involves many GUI and user interactions, manual testing may be a better
    way to test. 
\end{enumerate}

\end{document}